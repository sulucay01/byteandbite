# -*- coding: utf-8 -*-
"""IntentClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vmL2_RpEkPTMTdozixYzg4SHZzZ2vOPr
"""

# ============================================================
# 0. Environment + install
# ============================================================
import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

!pip install -q transformers datasets scikit-learn pandas

# ============================================================
# 1. Imports
# ============================================================
import random
from itertools import chain

import numpy as np
import pandas as pd
import torch
import torch.nn as nn

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    f1_score,
    precision_score,
    recall_score,
)

from transformers import (
    BertTokenizerFast,
    BertForSequenceClassification,
    TrainingArguments,
    Trainer,
)

from datasets import Dataset, DatasetDict

# ============================================================
# 2. Config
# ============================================================
DATA_PATH = "/content/restaurant_intents_dedup.jsonl"  # file name in Colab

TEXT_COLUMN = "text"
LABEL_COLUMN = "labels"        # list per row

MODEL_NAME = "bert-base-uncased"

MAX_LENGTH = 64
TEST_SIZE = 0.2
VAL_SIZE = 0.1
RANDOM_STATE = 42

TRAIN_BATCH_SIZE = 8
EVAL_BATCH_SIZE = 16

# >>> IMPORTANT CHANGES <<<
EPOCHS = 5                   # was 10
LEARNING_RATE = 2e-5         # was 2e-7 (too small, model barely learned)

THRESHOLD_FOR_METRICS = 0.5  # initial threshold; we'll tune after training

# ============================================================
# 3. Reproducibility
# ============================================================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(RANDOM_STATE)

# ============================================================
# 4. Load dataset (MULTI-LABEL)
# ============================================================
df = pd.read_json(DATA_PATH, lines=True)
print("Columns:", df.columns.tolist())
print(df.head())

df = df[[TEXT_COLUMN, LABEL_COLUMN]].dropna().reset_index(drop=True)

# Ensure labels are lists
df[LABEL_COLUMN] = df[LABEL_COLUMN].apply(
    lambda x: x if isinstance(x, list) else [x]
)

print("\nLabel combinations and counts:")
print(df[LABEL_COLUMN].value_counts())

def clean_text(s):
    return str(s).strip()

df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(clean_text)

# ============================================================
# 5. REMOVE DUPLICATES BEFORE SPLITTING
# ============================================================
print("\nRemoving duplicate (text, labels) rows BEFORE splitting...")

before = len(df)
df["_labels_tuple"] = df[LABEL_COLUMN].apply(tuple)
df = df.drop_duplicates(subset=[TEXT_COLUMN, "_labels_tuple"]).reset_index(drop=True)
after = len(df)

print(f"Removed {before - after} duplicate rows. New size = {after}")
df = df.drop(columns=["_labels_tuple"])

# ============================================================
# 6. Multi-label encoding -> multi-hot vectors
# ============================================================
all_tags = sorted(set(chain.from_iterable(df[LABEL_COLUMN])))
tag2id = {tag: i for i, tag in enumerate(all_tags)}
id2tag = {i: tag for tag, i in tag2id.items()}
num_labels = len(all_tags)

print("\nAll tags:", all_tags)
print("num_labels:", num_labels)

def encode_labels(label_list):
    vec = np.zeros(num_labels, dtype=int)
    for tag in label_list:
        if tag in tag2id:
            vec[tag2id[tag]] = 1
    return vec

df["label_vec"] = df[LABEL_COLUMN].apply(encode_labels)
print("\nEncoded sample:")
print(df[[TEXT_COLUMN, LABEL_COLUMN, "label_vec"]].head())

# ============================================================
# 7. Train / validation / test split
# ============================================================
train_val_df, test_df = train_test_split(
    df,
    test_size=TEST_SIZE,
    random_state=RANDOM_STATE,
)

relative_val_size = VAL_SIZE / (1 - TEST_SIZE)
train_df, val_df = train_test_split(
    train_val_df,
    test_size=relative_val_size,
    random_state=RANDOM_STATE,
)

print(f"\nSizes: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}")

# Sanity check: no exact duplicates across splits
train_tmp = train_df.copy()
test_tmp  = test_df.copy()
val_tmp   = val_df.copy()

train_tmp["_labels_tuple"] = train_tmp[LABEL_COLUMN].apply(tuple)
test_tmp["_labels_tuple"]  = test_tmp[LABEL_COLUMN].apply(tuple)
val_tmp["_labels_tuple"]   = val_tmp[LABEL_COLUMN].apply(tuple)

dupes_train_test = pd.merge(
    train_tmp[[TEXT_COLUMN, "_labels_tuple"]],
    test_tmp[[TEXT_COLUMN, "_labels_tuple"]],
    how="inner",
    on=[TEXT_COLUMN, "_labels_tuple"],
)
dupes_val_test = pd.merge(
    val_tmp[[TEXT_COLUMN, "_labels_tuple"]],
    test_tmp[[TEXT_COLUMN, "_labels_tuple"]],
    how="inner",
    on=[TEXT_COLUMN, "_labels_tuple"],
)

print("\n=== Sanity check: duplicates across splits ===")
print("Exact duplicates between train & test:", dupes_train_test.shape[0])
print("Exact duplicates between val   & test:", dupes_val_test.shape[0])

# ============================================================
# 8. Convert to HuggingFace Datasets
# ============================================================
train_dataset = Dataset.from_pandas(train_df[[TEXT_COLUMN, "label_vec"]])
val_dataset   = Dataset.from_pandas(val_df[[TEXT_COLUMN, "label_vec"]])
test_dataset  = Dataset.from_pandas(test_df[[TEXT_COLUMN, "label_vec"]])

train_dataset = train_dataset.rename_column("label_vec", "labels")
val_dataset   = val_dataset.rename_column("label_vec", "labels")
test_dataset  = test_dataset.rename_column("label_vec", "labels")

raw_datasets = DatasetDict(
    {"train": train_dataset, "validation": val_dataset, "test": test_dataset}
)
print("\nDatasets:", raw_datasets)

# ============================================================
# 9. Tokenizer & tokenization
# ============================================================
tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)

def tokenize_function(batch):
    return tokenizer(
        batch[TEXT_COLUMN],
        padding="max_length",
        truncation=True,
        max_length=MAX_LENGTH,
    )

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

def to_float(example):
    example["labels"] = [float(x) for x in example["labels"]]
    return example

tokenized_datasets = tokenized_datasets.map(to_float)
tokenized_datasets.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"],
)
print("\nTokenized datasets:", tokenized_datasets)

# ============================================================
# 10. Class weights for BCE (handle imbalance) â€“ CLIPPED
# ============================================================
label_matrix = np.stack(df["label_vec"].values)
label_counts = label_matrix.sum(axis=0)
total_samples = label_matrix.shape[0]

pos_weight = (total_samples - label_counts) / (label_counts + 1e-8)
pos_weight = torch.tensor(pos_weight, dtype=torch.float32)

# <<< IMPORTANT: clip to avoid insane weights for very rare labels >>>
pos_weight = torch.clamp(pos_weight, max=5.0)

print("\nLabel counts:", label_counts)
print("pos_weight (clipped):", pos_weight)

# ============================================================
# 11. Model
# ============================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("\nDevice:", device)

model = BertForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=num_labels,
)
model.config.problem_type = "multi_label_classification"
model.to(device)

# ============================================================
# 12. Metrics for multi-label classification (MICRO + MACRO)
# ============================================================
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probs = 1 / (1 + np.exp(-logits))  # sigmoid
    y_pred = (probs >= THRESHOLD_FOR_METRICS).astype(int)

    micro_f1 = f1_score(labels, y_pred, average="micro", zero_division=0)
    macro_f1 = f1_score(labels, y_pred, average="macro", zero_division=0)
    subset_acc = (y_pred == labels).all(axis=1).mean()

    return {
        "subset_accuracy": subset_acc,
        "micro_f1": micro_f1,
        "macro_f1": macro_f1,
    }

# ============================================================
# 13. TrainingArguments  (no eval_strategy to avoid version issues)
# ============================================================
training_args = TrainingArguments(
    output_dir="./bert-intent-ml-checkpoints",
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=EVAL_BATCH_SIZE,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=20,
    save_steps=500,
    save_total_limit=2,
    fp16=torch.cuda.is_available(),
    max_grad_norm=1.0,
)

# ============================================================
# 14. Custom Trainer with BCEWithLogitsLoss (multi-label)
# ============================================================
class MultiLabelTrainer(Trainer):
    def __init__(self, pos_weight=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pos_weight = pos_weight

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        labels = labels.to(logits.dtype)

        if self.pos_weight is not None:
            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))
        else:
            loss_fct = nn.BCEWithLogitsLoss()

        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

trainer = MultiLabelTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    pos_weight=pos_weight,
)

# ============================================================
# 15. Train
# ============================================================
trainer.train()

# ============================================================
# 16. Threshold tuning on validation set
# ============================================================
print("\n=== Threshold tuning on validation set ===")
val_pred = trainer.predict(tokenized_datasets["validation"])
val_logits = val_pred.predictions
val_labels = val_pred.label_ids
val_probs = 1 / (1 + np.exp(-val_logits))

best_thr = None
best_micro_f1 = -1

for thr in [0.2, 0.3, 0.4, 0.5, 0.6]:
    y_pred_thr = (val_probs >= thr).astype(int)
    micro_f1 = f1_score(val_labels, y_pred_thr, average="micro", zero_division=0)
    print(f"Threshold {thr:.2f} -> micro_f1 = {micro_f1:.4f}")
    if micro_f1 > best_micro_f1:
        best_micro_f1 = micro_f1
        best_thr = thr

print(f"\nBest threshold on validation: {best_thr:.2f} (micro_f1={best_micro_f1:.4f})")
THRESHOLD_FOR_METRICS = best_thr  # update global threshold

# ============================================================
# 17. Evaluate on validation & test sets with tuned threshold
# ============================================================
def evaluate_with_threshold(dataset, probs, labels, thr):
    y_pred = (probs >= thr).astype(int)
    micro_f1 = f1_score(labels, y_pred, average="micro", zero_division=0)
    macro_f1 = f1_score(labels, y_pred, average="macro", zero_division=0)
    subset_acc = (y_pred == labels).all(axis=1).mean()
    return micro_f1, macro_f1, subset_acc

print("\n=== Validation results (tuned threshold) ===")
val_micro, val_macro, val_subset = evaluate_with_threshold(
    tokenized_datasets["validation"],
    val_probs,
    val_labels,
    THRESHOLD_FOR_METRICS,
)
print(f"micro_f1  = {val_micro:.4f}")
print(f"macro_f1  = {val_macro:.4f}")
print(f"subset_acc= {val_subset:.4f}")

print("\n=== Test results (tuned threshold) ===")
test_pred = trainer.predict(tokenized_datasets["test"])
test_logits = test_pred.predictions
test_labels = test_pred.label_ids
test_probs = 1 / (1 + np.exp(-test_logits))

test_micro, test_macro, test_subset = evaluate_with_threshold(
    tokenized_datasets["test"],
    test_probs,
    test_labels,
    THRESHOLD_FOR_METRICS,
)
print(f"micro_f1  = {test_micro:.4f}")
print(f"macro_f1  = {test_macro:.4f}")
print(f"subset_acc= {test_subset:.4f}")

# Per-label F1 on test
y_pred_test = (test_probs >= THRESHOLD_FOR_METRICS).astype(int)
print("\n=== Per-label F1 scores (test set) ===")
for i, tag in id2tag.items():
    f1 = f1_score(test_labels[:, i], y_pred_test[:, i], zero_division=0)
    print(f"{tag:20} F1 = {f1:.3f}")

# ============================================================
# 18. Helper: debug probabilities for one text
# ============================================================
def debug_probs(text, model=model, tokenizer=tokenizer, max_length=MAX_LENGTH):
    model.eval()
    enc = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    ).to(model.device)

    with torch.no_grad():
        logits = model(**enc).logits
        probs = torch.sigmoid(logits).cpu().numpy()[0]

    for idx, p in enumerate(probs):
        print(f"{id2tag[idx]:20} -> {p:.3f}")

print("\n=== Debug example ===")
debug_probs("looking for seafood in Los Angeles's Koreatown")

# ============================================================
# 19. Inference: predict_intent (uses tuned threshold by default)
# ============================================================
def predict_intent(
    texts,
    model=model,
    tokenizer=tokenizer,
    max_length=MAX_LENGTH,
    threshold=None,
    top_k=None,
):
    """
    Multi-label inference.
    - If threshold is given: select all labels with prob >= threshold.
    - Else if top_k is given: select top_k labels.
    - Else: use globally tuned THRESHOLD_FOR_METRICS.
    """
    model.eval()
    if isinstance(texts, str):
        texts = [texts]

    encodings = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    ).to(model.device)

    with torch.no_grad():
        logits = model(**encodings).logits
        probs = torch.sigmoid(logits).cpu().numpy()

    results = []
    for p in probs:
        thr = threshold if threshold is not None else THRESHOLD_FOR_METRICS
        if thr is not None:
            idx = np.where(p >= thr)[0]
        elif top_k is not None:
            idx = np.argsort(p)[-top_k:][::-1]
        else:
            idx = np.where(p >= 0.5)[0]

        labels = [id2tag[i] for i in idx]
        confs = p[idx].tolist()
        results.append((labels, confs))

    return results

print("\n=== Quick prediction examples ===")
examples = [
    "cheap vegan pizza places in Boston",
    "book me a flight to Paris",
    "best Italian restaurants in Dallas",
]

for t, (labels, confs) in zip(examples, predict_intent(examples)):
    print(f"TEXT: {t}")
    for lab, c in zip(labels, confs):
        print(f"  -> {lab} ({c:.3f})")
    print()

# ============================================================
# 20. Save model & tokenizer
# ============================================================
SAVE_DIR = "bert_intent_multilabel_model"
os.makedirs(SAVE_DIR, exist_ok=True)

model.save_pretrained(SAVE_DIR)
tokenizer.save_pretrained(SAVE_DIR)
np.save(f"{SAVE_DIR}/all_tags.npy", np.array(all_tags))

print(f"\nModel, tokenizer, and label list saved to: {SAVE_DIR}")

# ======== MANUAL TESTING WITH NEW EXAMPLES ========

# Put any texts you want to test here
new_examples = [
    "can you find me a vegan restuarant in los angeles. I need help for my homework"
]

# Use threshold or top_k (you can change these)
THRESH = 0.5   # or e.g. 0.4
TOP_K = None   # or e.g. 3 if you want exactly 3 labels

results = predict_intent(
    new_examples,
    model=model,
    tokenizer=tokenizer,
    threshold=THRESH,
    top_k=TOP_K,
)

for text, (labels, confs) in zip(new_examples, results):
    print("=" * 80)
    print("TEXT:", text)
    if not labels:
        print("  -> no labels above threshold")
    else:
        for lab, c in zip(labels, confs):
            print(f"  -> {lab:20} ({c:.3f})")

