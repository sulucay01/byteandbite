# Model Card: BERT Intent Classifier

## Model Details

### Person or organization developing model
Experiment Team

### Model date
2025-11-02

### Model version
1.0

### Model type
Sequence Classification (DistilBERT-based)

### Information about training algorithms, parameters, fairness constraints or other applied approaches, and features
- Architecture: DistilBERT (distilbert-base-uncased)
- Training Algorithm: Fine-tuning with AdamW optimizer
- Learning Rate: 5e-5
- Batch Size: 8
- Weight Decay: 0.01
- Max Sequence Length: 64 tokens
- Early Stopping: Enabled (patience=3)
- Features: Raw text input, tokenized with DistilBERT tokenizer

### Paper or other resource for more information
- DistilBERT: https://arxiv.org/abs/1910.01108
- Hugging Face Transformers: https://huggingface.co/docs/transformers

### License
Check base model license (DistilBERT)

### Where to send questions or comments about the model
Contact experiment team

## Intended Use

### Primary intended uses
- Intent classification for food & restaurant chatbot
- Preprocessing step to route queries to appropriate response handlers
- Integration with LLM-based question answering systems

### Primary intended users
- Chatbot developers
- Food & restaurant service providers
- NLP researchers

### Out-of-scope use cases
- General purpose intent classification beyond restaurant/food domain
- Direct question answering (requires downstream LLM)
- Multi-turn conversation understanding

## Factors

### Relevant factors
- Query language (English)
- Food & restaurant domain
- Intent type (e.g., menu inquiry, reservation, dietary restrictions)

### Evaluation factors
- Intent class distribution
- Query length and complexity
- Similar intent classes (potential confusion)

## Metrics

### Model performance measures
N/A - Model metrics not available

### Decision thresholds
- Classification: Argmax over logits
- Confidence threshold: Not explicitly set (can be added for filtering low-confidence predictions)

### Variation approaches
- Stratified train-test split (80/20)
- Evaluation on held-out test set
- Early stopping to prevent overfitting

## Evaluation Data

### Datasets
- Test set: 20% stratified split from intents.json patterns
- Number of intent classes: 12

### Motivation
Stratified split ensures balanced representation of all intent classes in evaluation.

### Preprocessing
- Tokenization with DistilBERT tokenizer
- Max sequence length: 64 tokens
- Padding and truncation applied

## Training Data

### Details
- Training set: 80% stratified split from intents.json patterns
- Intent classes: general_intent, not_allowed, available_tables, hours, menu, contact, address, offers, vegan_enquiry, veg_enquiry, ... (total: 12 classes)
- Label encoding: Standard label encoder mapping classes to integers
- Preprocessing: Same as evaluation data (tokenization, padding, truncation)

## Quantitative Analyses

### Unitary results
See performance metrics above.

### Intersectional results
Not applicable for this intent classification task.

## Ethical Considerations
- Model is trained on domain-specific patterns for food & restaurant queries
- No sensitive demographic information is used
- Model should not be used to discriminate or bias responses based on user characteristics
- Intent classification is a preprocessing step; final responses are generated by LLM components

## Caveats and Recommendations
- Model accuracy depends on quality and coverage of training patterns in intents.json
- May struggle with out-of-vocabulary queries or novel phrasings
- Should be combined with LLM fallback for robust handling
- Regular retraining recommended as new intents or patterns are added
- Consider confidence threshold for filtering uncertain predictions
